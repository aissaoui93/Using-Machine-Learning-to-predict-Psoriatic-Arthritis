{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note: before we start, it's good to remind the reader that this notebook is used to predict PsO: the skin disease called Psoraisis, based on phenotype data of 86499 patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Data Importation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('Ahmed_Data/train.csv')\n",
    "test_data  = pd.read_csv('Ahmed_Data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the size if the training data set is  (73481, 78)\n",
      "the size if the test data set is  (73481, 78)\n"
     ]
    }
   ],
   "source": [
    "print('the size if the training data set is ', train_data.shape)\n",
    "print('the size if the test data set is ', train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJID</th>\n",
       "      <th>BMI</th>\n",
       "      <th>ALCOHOL_STATUS</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>REVISED_RPGEHRACE</th>\n",
       "      <th>SMOKING_STATUS</th>\n",
       "      <th>a1c_MEAN</th>\n",
       "      <th>a1c_UNDER</th>\n",
       "      <th>a1c_OVER</th>\n",
       "      <th>MEAN_bun</th>\n",
       "      <th>...</th>\n",
       "      <th>IBD</th>\n",
       "      <th>METABOLIC</th>\n",
       "      <th>MS</th>\n",
       "      <th>MYO_INFARC</th>\n",
       "      <th>PSORIASIS</th>\n",
       "      <th>PSOR_ARTH</th>\n",
       "      <th>RHEUM_ARTH</th>\n",
       "      <th>SCLERODERMA</th>\n",
       "      <th>SLE</th>\n",
       "      <th>VITILIGO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>158108025613</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>other/uncertain</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.140000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>283455485374</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>white</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.571429</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>231997356863</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>white</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.183333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>685384313765</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>hispanic</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.462069</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.854167</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>229256381841</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>white</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.705000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>13.461538</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         SUBJID   BMI  ALCOHOL_STATUS  GENDER REVISED_RPGEHRACE  \\\n",
       "0  158108025613  23.0             1.0       2   other/uncertain   \n",
       "1  283455485374  32.0             1.0       2             white   \n",
       "2  231997356863  37.0             0.0       2             white   \n",
       "3  685384313765  31.0             1.0       1          hispanic   \n",
       "4  229256381841  24.0             0.0       1             white   \n",
       "\n",
       "   SMOKING_STATUS  a1c_MEAN  a1c_UNDER  a1c_OVER   MEAN_bun    ...     IBD  \\\n",
       "0             1.0  5.140000        0.0  0.000000        NaN    ...       0   \n",
       "1             1.0  6.100000        0.0  1.000000  15.571429    ...       0   \n",
       "2             1.0  6.183333        0.0  0.333333  15.000000    ...       0   \n",
       "3             2.0  5.462069        0.0  0.000000  35.854167    ...       0   \n",
       "4             1.0  5.705000        0.0  0.050000  13.461538    ...       0   \n",
       "\n",
       "   METABOLIC  MS  MYO_INFARC  PSORIASIS  PSOR_ARTH  RHEUM_ARTH  SCLERODERMA  \\\n",
       "0          0   0           0          0          0           0            0   \n",
       "1          0   0           0          0          0           0            0   \n",
       "2          0   0           0          0          0           0            0   \n",
       "3          0   0           1          0          0           0            0   \n",
       "4          0   0           0          0          0           0            0   \n",
       "\n",
       "   SLE  VITILIGO  \n",
       "0    0         0  \n",
       "1    0         0  \n",
       "2    0         0  \n",
       "3    0         0  \n",
       "4    0         0  \n",
       "\n",
       "[5 rows x 78 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Further Data Preprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Column \"BMI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 557 missing values in the BMI column in the training data and 103  in the test data. We will replace all of them by the mean value of all the pateints\n"
     ]
    }
   ],
   "source": [
    "# Let's find the number of the missing values in ALCOHOL_STATUS\n",
    "print( 'There are', train_data[train_data.BMI.isnull()].shape[0], 'missing values in the BMI column in the training', \n",
    "       'data and', test_data[test_data.BMI.isnull()].shape[0], ' in the test data. We will replace all of them by '\n",
    "       'the mean value of all the pateints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing the missing values in the BMI column by the mean value\n",
    "train_data.BMI.fillna(train_data.BMI.mean(), inplace=True)\n",
    "test_data.BMI.fillna(test_data.BMI.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Column \"ALCOHOL_STATUS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    43313\n",
       "0.0    27664\n",
       "Name: ALCOHOL_STATUS, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.ALCOHOL_STATUS.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of missing values in ALCOHOL_STATUS is  2504\n"
     ]
    }
   ],
   "source": [
    "# Let's find the number of the missing values in ALCOHOL_STATUS\n",
    "print( 'the number of missing values in ALCOHOL_STATUS is ', train_data.shape[0] - train_data.ALCOHOL_STATUS.value_counts()[0] - train_data.ALCOHOL_STATUS.value_counts()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: the SVM doesn't handle missing values. We will need to treat all of them before training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPLACING MISSING VALUES\n",
    "# SINCE 0=not a drinker, 1=drinker, we add 2= missing information \n",
    "train_data.ALCOHOL_STATUS.fillna(2, inplace=True)\n",
    "test_data.ALCOHOL_STATUS.fillna(2, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Column \"REVISED_RPGEHRACE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "white              59502\n",
       "asian               6066\n",
       "hispanic            5041\n",
       "black               2329\n",
       "other/uncertain      543\n",
       "Name: REVISED_RPGEHRACE, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.REVISED_RPGEHRACE.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since some ML algorithms only accept numeric input, we translate the strings into digits\n",
    "train_data.REVISED_RPGEHRACE.replace(['white', 'asian','hispanic','black','other/uncertain'], [1,2,3,4,5], inplace=True)\n",
    "test_data.REVISED_RPGEHRACE.replace(['white', 'asian','hispanic','black','other/uncertain'], [1,2,3,4,5], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Column \"SMOKING_STATUS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3109 missing values in SMOKING_STATUS \n"
     ]
    }
   ],
   "source": [
    "# Let's find the number of the missing values in SMOKING_STATUS\n",
    "print( 'There are', train_data.shape[0] - train_data.SMOKING_STATUS.value_counts()[1] - train_data.SMOKING_STATUS.value_counts()[2] - train_data.SMOKING_STATUS.value_counts()[3], 'missing values in SMOKING_STATUS ') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPLACING MISSING VALUES\n",
    "# Since 1=Never, 2=Former, 3=Current, we add 4=missing information\n",
    "train_data.SMOKING_STATUS.fillna(4, inplace=True)\n",
    "test_data.SMOKING_STATUS.fillna(4, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columns 'a1c_MEAN', 'a1c_UNDER', 'a1c_OVER', 'MEAN_bun', 'UNDER_bun',... that have been created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: Those columns contain some missing values that are generated from the missing values in the test results for some patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_col = ['a1c_MEAN', 'a1c_UNDER', 'a1c_OVER', 'MEAN_bun',\n",
    "       'UNDER_bun', 'OVER_bun', 'ccp_MEAN', 'ccp_UNDER', 'ccp_OVER',\n",
    "       'MEAN_crea', 'UNDER_crea', 'OVER_crea', 'MEAN_CRP', 'OVER_CRP',\n",
    "       'MEAN_ESR', 'OVER_ESR', 'MEAN_GLUC', 'UNDER_GLUC', 'OVER_GLUC',\n",
    "       'MEAN_RHEUMATOID', 'UNDER_RHEUMATOID', 'OVER_RHEUMATOID',\n",
    "       'v_d_MEAN', 'v_d_UNDER', 'v_d_OVER', 'Age', 'HDL_Normal',\n",
    "       'HDL_Optimal', 'LDL_Optimal', 'LDL_Low_Risk', 'LDL_Normal_Risk',\n",
    "       'LDL_High_Risk', 'LDL_Very_High_Risk', 'TRIGLYC_Optimal',\n",
    "       'TRIGLYC_Low_Risk', 'TRIGLYC_Normal_Risk', 'TRIGLYC_High_Risk',\n",
    "       'TRIGLYC_Very_High_Risk', 'ADALIMUMAB_RX', 'LEFLUNOMIDE_FILLS',\n",
    "       'METHOTREXATE_FILLS', 'ADALIMUMAB_FILLS', 'USTEKINUMAB_FILLS',\n",
    "       'CYCLOSPORINE_FILLS', 'ETANERCEPT_FILLS', 'INFLIXIMAB_FILLS',\n",
    "       'ETANERCEPT_RX', 'ACITRETIN_RX', 'CYCLOSPORINE_RX',\n",
    "       'ANTIRHEUMATIC_RX', 'ANTIRHEUMATIC_FILLS', 'LEFLUNOMIDE_RX',\n",
    "       'USTEKINUMAB_RX', 'METHOTREXATE_RX', 'INFLIXIMAB_RX',\n",
    "       'ACITRETIN_FILLS']\n",
    "for i in list_col:\n",
    "    mean_train = train_data[i].mean()\n",
    "    mean_test = train_data[i].mean()\n",
    "    train_data[i].fillna(mean_train, inplace=True)\n",
    "    test_data[i].fillna(mean_test, inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. First Model: k Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: There is no need here to have a validation set and a training set since we are using cross validation in the grid search function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train =  train_data.drop(['PSORIASIS', 'SUBJID'], axis=1)\n",
    "Y_train =  train_data.PSORIASIS\n",
    "\n",
    "X_test =  test_data.drop(['PSORIASIS', 'SUBJID'], axis=1)\n",
    "Y_test =  test_data.PSORIASIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tuning the model : finding the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'n_neighbors': array([1, 2, 3, 4, 5])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# we try values of K (nearest neighbors) from 1 to 5\n",
    "k = np.arange(5)+1\n",
    "parameters = {'n_neighbors' : k}\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# we use GridSearch wit a cross validation of 10 folds to find the best parameter K\n",
    "knn_GridSearch = GridSearchCV(knn, parameters, cv=5)\n",
    "\n",
    "# fit nearest neighbors to our training dataset\n",
    "knn_GridSearch.fit(X_train,Y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 4}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GridSearch give us the best parameter K:\n",
    "knn_GridSearch.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training the model using the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=4, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=4)\n",
    "knn.fit(X_train,Y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Assessing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9398519432449105"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just for a simple evaluation, we could use the sklearn built-in method \"score\"\n",
    "# Score method returns the mean accuracy on the given test data and labels\n",
    "knn.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import model_selection\n",
    "\n",
    "seed = 0\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "knn = KNeighborsClassifier(n_neighbors=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: In this project, we will use diffrent classification evaluation metrics: \n",
    "\n",
    "##### 1. Classification Accuracy\n",
    "\n",
    "Classification accuracy is the ratio (number of correct predictions / all predictions made).\n",
    "\n",
    "Good to know about the metric:\n",
    "- The most common evaluation metric for classification problems.\n",
    "- Only suitable when there are an equal number of observations in each class (which is not the case in our unbalanced dataset) \n",
    "- Only suitable when prediction errors are equally important, which is often not the case here: False Negatives are clearly worse than False Positives in our situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Accuracy: 0.938 (0.002)\n"
     ]
    }
   ],
   "source": [
    "## Classification Accuracy\n",
    "scoring = 'accuracy'\n",
    "results = model_selection.cross_val_score(knn, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "print(\"- Accuracy: %.3f (%.3f)\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Logarithmic Loss\n",
    "\n",
    "Logarithmic loss (or logloss) is a performance metric for evaluating the predictions of probabilities of membership to a given class.\n",
    "\n",
    "Good to know about the metric:\n",
    "- The scalar probability between 0 and 1 can be seen as a measure of confidence for a prediction by an algorithm. Predictions that are correct or incorrect are rewarded or punished proportionally to the confidence of the prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Logloss: -1.657 (0.069)\n"
     ]
    }
   ],
   "source": [
    "## Logarithmic Loss\n",
    "scoring = 'neg_log_loss'\n",
    "results = model_selection.cross_val_score(knn, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "print(\"- Logloss: %.3f (%.3f)\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Area under ROC Curve\n",
    "\n",
    "The AUC represents a model’s ability to discriminate between positive and negative classes. An area of 1.0 represents a model that made all predictions perfectly. An area of 0.5 represents a model as good as random. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- AUC: 0.539 (0.010)\n"
     ]
    }
   ],
   "source": [
    "## Area Under ROC Curve\n",
    "scoring = 'roc_auc'\n",
    "results = model_selection.cross_val_score(knn, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "print(\"- AUC: %.3f (%.3f)\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Confusion Matrix (or error matrix)\n",
    "\n",
    "It is a specific table layout that allows visualization of the performance of an algorithm. Each row of the matrix represents the instances in a predicted class. It is a special kind of contingency table, with two dimensions (\"actual\" and \"predicted\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Confusion Matrix: \n",
      "[[12166    19]\n",
      " [  761    22]]\n"
     ]
    }
   ],
   "source": [
    "## Cross Validation Classification Confusion Matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "knn.fit(X_train, Y_train)\n",
    "predicted = knn.predict(X_test)\n",
    "matrix = confusion_matrix(Y_test, predicted)\n",
    "print(\"- Confusion Matrix: \")\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Second Model: Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "GNB = GaussianNB()\n",
    "GNB.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of mislabeld data in test set\n",
    "y_pred = GNB.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 12968 points : 651\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (X_test.shape[0],(Y_test != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94979950647748301"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns the mean accuracy on the given test data and labels\n",
    "GNB.score(X_test, Y_test, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7338690467\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "print(log_loss(Y_test, GNB.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Accuracy: 0.947 (0.002)\n",
      "- Logloss: -0.223 (0.009)\n",
      "- AUC: 0.678 (0.013)\n",
      "- Confusion Matrix: \n",
      "[[12099    86]\n",
      " [  581   202]]\n"
     ]
    }
   ],
   "source": [
    "## Logarithmic Loss\n",
    "from sklearn import model_selection\n",
    "\n",
    "seed = 7\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "GNB = GaussianNB()\n",
    "\n",
    "\n",
    "## Classification Accuracy\n",
    "scoring = 'accuracy'\n",
    "results = model_selection.cross_val_score(GNB, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "print(\"- Accuracy: %.3f (%.3f)\" % (results.mean(), results.std()))\n",
    "\n",
    "## Logarithmic Loss\n",
    "scoring = 'neg_log_loss'\n",
    "results = model_selection.cross_val_score(GNB, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "print(\"- Logloss: %.3f (%.3f)\" % (results.mean(), results.std()))\n",
    "\n",
    "## Area Under ROC Curve\n",
    "scoring = 'roc_auc'\n",
    "results = model_selection.cross_val_score(GNB, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "print(\"- AUC: %.3f (%.3f)\" % (results.mean(), results.std()))\n",
    "\n",
    "## Cross Validation Classification Confusion Matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "GNB.fit(X_train, Y_train)\n",
    "predicted = GNB.predict(X_test)\n",
    "matrix = confusion_matrix(Y_test, predicted)\n",
    "print(\"- Confusion Matrix: \")\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Tuning the model : finding the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'alpha': array([  1,   2, ...,  99, 100])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# we try values of K (nearest neighbors) from 1 to 5\n",
    "alpha = np.arange(100)+1\n",
    "parameters = {'alpha' : alpha}\n",
    "MNB = MultinomialNB()\n",
    "\n",
    "# we use GridSearch wit a cross validation of 10 folds to find the best parameter K\n",
    "MNB_GridSearch = GridSearchCV(MNB, parameters, cv=10)\n",
    "\n",
    "# fit nearest neighbors to our training dataset\n",
    "MNB_GridSearch.fit(X_train,Y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 58}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNB_GridSearch.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Training the model using the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=58.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "MNB = MultinomialNB(alpha=58.0, fit_prior=True, class_prior=None)\n",
    "MNB.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Assessing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 12968 points : 667\n"
     ]
    }
   ],
   "source": [
    "# number of mislabeld data in test set\n",
    "y_pred = MNB.predict(X_test)\n",
    "\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (X_test.shape[0],(Y_test != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94856570018507091"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns the mean accuracy on the given test data and labels\n",
    "MNB.score(X_test, Y_test, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.77648308303\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "print(log_loss(Y_test, MNB.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Accuracy: 0.947 (0.002)\n",
      "- Logloss: -0.223 (0.009)\n",
      "- AUC: 0.678 (0.013)\n",
      "- Confusion Matrix: \n",
      "[[12099    86]\n",
      " [  581   202]]\n"
     ]
    }
   ],
   "source": [
    "## Logarithmic Loss\n",
    "from sklearn import model_selection\n",
    "\n",
    "seed = 7\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "MNB = MultinomialNB(alpha=58.0, fit_prior=True, class_prior=None)\n",
    "\n",
    "\n",
    "## Classification Accuracy\n",
    "scoring = 'accuracy'\n",
    "results = model_selection.cross_val_score(MNB, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "print(\"- Accuracy: %.3f (%.3f)\" % (results.mean(), results.std()))\n",
    "\n",
    "## Logarithmic Loss\n",
    "scoring = 'neg_log_loss'\n",
    "results = model_selection.cross_val_score(MNB, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "print(\"- Logloss: %.3f (%.3f)\" % (results.mean(), results.std()))\n",
    "\n",
    "## Area Under ROC Curve\n",
    "scoring = 'roc_auc'\n",
    "results = model_selection.cross_val_score(MNB, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "print(\"- AUC: %.3f (%.3f)\" % (results.mean(), results.std()))\n",
    "\n",
    "## Cross Validation Classification Confusion Matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "MNB.fit(X_train, Y_train)\n",
    "predicted = MNB.predict(X_test)\n",
    "matrix = confusion_matrix(Y_test, predicted)\n",
    "print(\"- Confusion Matrix: \")\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Third Model : Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: Here we use Support Vector Machine” (SVM) as a supervised machine learning algorithm in a purpose of classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, Y_train, Y_valid = train_test_split(train_data.drop(['PSORIASIS', 'SUBJID'], axis=1),\n",
    "                                                      train_data.PSORIASIS,\n",
    "                                                      test_size=0.33, random_state=42)\n",
    "\n",
    "X_test =  test_data.drop(['PSORIASIS', 'SUBJID'], axis=1)\n",
    "Y_test =  test_data.PSORIASIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: One of the disadvatages of SVM is that it is computationally expensive, thus runs slow. It is then not time-efficient to run a grid search in order to find the best parameters. As a result, I will run diffrent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Training the model with diffrent parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93777062971668934"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train an SVM classification model with C=1 and kernel = rbf \"radial basis function\"\n",
    "\n",
    "svm_1 = sklearn.svm.SVC(C=1, kernel='rbf')\n",
    "svm_1.fit(X_train,Y_train)\n",
    "svm_1.score(X_valid, Y_valid, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93146109117901765"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train an SVM classification model with C=10 and kernel = rbf \"radial basis function\"\n",
    "\n",
    "svm_2 = sklearn.svm.SVC(C=10, kernel='rbf')\n",
    "svm_2.fit(X_train,Y_train)\n",
    "svm_2.score(X_valid, Y_valid, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93789434615860445"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train an SVM classification model with C=0.1 and kernel = rbf \"radial basis function\"\n",
    "\n",
    "svm_3 = sklearn.svm.SVC(C=0.1, kernel='rbf')\n",
    "svm_3.fit(X_train,Y_train)\n",
    "svm_3.score(X_valid, Y_valid, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93789434615860445"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train an SVM classification model with C=0.01 and kernel = rbf \"radial basis function\"\n",
    "\n",
    "svm_4 = sklearn.svm.SVC(C=0.01, kernel='rbf')\n",
    "svm_4.fit(X_train,Y_train)\n",
    "svm_4.score(X_valid, Y_valid, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91694502866097571"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train an SVM classification model with C=100 and kernel = rbf \"radial basis function\"\n",
    "\n",
    "svm_5 = sklearn.svm.SVC(C=100, kernel='rbf')\n",
    "svm_5.fit(X_train,Y_train)\n",
    "svm_5.score(X_valid, Y_valid, sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Evaluation the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "seed = 7\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "svm = sklearn.svm.SVC(C=0.1, kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Accuracy: 0.939 (0.002)\n"
     ]
    }
   ],
   "source": [
    "## Classification Accuracy\n",
    "scoring = 'accuracy'\n",
    "results = model_selection.cross_val_score(svm, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "print(\"- Accuracy: %.3f (%.3f)\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logarithmic Loss\n",
    "scoring = 'neg_log_loss'\n",
    "results = model_selection.cross_val_score(svm, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "print(\"- Logloss: %.3f (%.3f)\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Area Under ROC Curve\n",
    "scoring = 'roc_auc'\n",
    "results = model_selection.cross_val_score(svm, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "print(\"- AUC: %.3f (%.3f)\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cross Validation Classification Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "svm.fit(X_train, Y_train)\n",
    "predicted = svm.predict(X_test)\n",
    "matrix = confusion_matrix(Y_test, predicted)\n",
    "print(\"- Confusion Matrix: \")\n",
    "print(matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda3",
   "language": "python",
   "name": "anaconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
